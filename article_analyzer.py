import openai
import pandas as pd
import numpy as np
from sklearn.cluster import KMeans
from sklearn.metrics.pairwise import cosine_similarity
import json
from datetime import datetime
import sys
import os

# supabase_client import
from supabase_client import init_supabase

class BlindSpotAnalyzer:
    def __init__(self, openai_api_key):
        """BlindSpot ê¸°ì‚¬ ë¶„ì„ê¸° ì´ˆê¸°í™”"""
        self.openai_client = openai.OpenAI(api_key=openai_api_key)
        self.supabase = init_supabase()
        print("ğŸ¤– OpenAI í´ë¼ì´ì–¸íŠ¸ ë° Supabase ì—°ê²° ì™„ë£Œ")
        
    def load_articles_from_db(self):
        """Supabaseì—ì„œ ëª¨ë“  ê¸°ì‚¬ ë°ì´í„° ë¡œë“œ"""
        try:
            # ê¸°ì‚¬ì™€ ì–¸ë¡ ì‚¬, ì¹´í…Œê³ ë¦¬ ì •ë³´ë¥¼ JOINí•´ì„œ ê°€ì ¸ì˜¤ê¸°
            response = self.supabase.table('articles').select("""
                id,
                title,
                content,
                url,
                published_at,
                media_outlets(name, bias),
                categories(name)
            """).execute()
            
            print(f"ğŸ“Š ì´ {len(response.data)}ê°œ ê¸°ì‚¬ ë¡œë“œ ì™„ë£Œ")
            return response.data
            
        except Exception as e:
            print(f"âŒ ê¸°ì‚¬ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return []
    
    def get_embeddings(self, texts, model="text-embedding-ada-002"):
        """OpenAI Embeddings APIë¡œ í…ìŠ¤íŠ¸ ë²¡í„°í™”"""
        try:
            # í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¸¸ë©´ ìë¥´ê¸° (í† í° ì œí•œ)
            processed_texts = []
            for text in texts:
                # ì œëª©ê³¼ ë³¸ë¬¸ ì•ë¶€ë¶„ë§Œ ì‚¬ìš© (ì•½ 8000ì ì œí•œ)
                truncated = text[:8000] if len(text) > 8000 else text
                processed_texts.append(truncated)
            
            print(f"ğŸ”„ {len(processed_texts)}ê°œ í…ìŠ¤íŠ¸ì˜ ì„ë² ë”© ìƒì„± ì¤‘...")
            
            response = self.openai_client.embeddings.create(
                model=model,
                input=processed_texts
            )
            
            embeddings = [data.embedding for data in response.data]
            print(f"âœ… ì„ë² ë”© ìƒì„± ì™„ë£Œ: {len(embeddings)}ê°œ")
            
            return np.array(embeddings)
            
        except Exception as e:
            print(f"âŒ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            return None
    
    def cluster_articles(self, articles, n_clusters=8):
        """ê¸°ì‚¬ë“¤ì„ ì£¼ì œë³„ë¡œ í´ëŸ¬ìŠ¤í„°ë§"""
        print(f"\nğŸ¯ {len(articles)}ê°œ ê¸°ì‚¬ë¥¼ {n_clusters}ê°œ í´ëŸ¬ìŠ¤í„°ë¡œ ë¶„ë¥˜ ì¤‘...")
        
        # ê¸°ì‚¬ í…ìŠ¤íŠ¸ ì¤€ë¹„ (ì œëª© + ë³¸ë¬¸ ì•ë¶€ë¶„)
        texts = []
        for article in articles:
            title = article.get('title', '')
            content = article.get('content', '')
            # ì œëª©ê³¼ ë³¸ë¬¸ ì• 500ìë¥¼ í•©ì³ì„œ ì‚¬ìš©
            combined_text = f"{title}\n\n{content[:500]}"
            texts.append(combined_text)
        
        # OpenAI ì„ë² ë”© ìƒì„±
        embeddings = self.get_embeddings(texts)
        if embeddings is None:
            return None
        
        # K-means í´ëŸ¬ìŠ¤í„°ë§
        kmeans = KMeans(n_clusters=n_clusters, random_state=42)
        cluster_labels = kmeans.fit_predict(embeddings)
        
        # ê²°ê³¼ ì •ë¦¬
        clustered_articles = []
        for i, article in enumerate(articles):
            article_with_cluster = article.copy()
            article_with_cluster['cluster_id'] = int(cluster_labels[i])
            article_with_cluster['embedding'] = embeddings[i].tolist()
            clustered_articles.append(article_with_cluster)
        
        print(f"âœ… í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ!")
        return clustered_articles, kmeans.cluster_centers_
    
    def analyze_cluster_topics(self, clustered_articles):
        """GPTë¥¼ ì‚¬ìš©í•´ì„œ ê° í´ëŸ¬ìŠ¤í„°ì˜ ì£¼ì œ ë¶„ì„"""
        print(f"\nğŸ“ í´ëŸ¬ìŠ¤í„°ë³„ ì£¼ì œ ë¶„ì„ ì¤‘...")
        
        # í´ëŸ¬ìŠ¤í„°ë³„ë¡œ ê¸°ì‚¬ ê·¸ë£¹í™”
        clusters = {}
        for article in clustered_articles:
            cluster_id = article['cluster_id']
            if cluster_id not in clusters:
                clusters[cluster_id] = []
            clusters[cluster_id].append(article)
        
        cluster_topics = {}
        
        for cluster_id, articles in clusters.items():
            print(f"í´ëŸ¬ìŠ¤í„° {cluster_id} ë¶„ì„ ì¤‘... ({len(articles)}ê°œ ê¸°ì‚¬)")
            
            # í´ëŸ¬ìŠ¤í„°ì˜ ëŒ€í‘œ ê¸°ì‚¬ ì œëª©ë“¤
            titles = [article['title'] for article in articles[:10]]  # ìƒìœ„ 10ê°œë§Œ
            titles_text = "\n".join([f"- {title}" for title in titles])
            
            # GPTì—ê²Œ ì£¼ì œ ë¶„ì„ ìš”ì²­
            prompt = f"""
ë‹¤ìŒì€ ë‰´ìŠ¤ ê¸°ì‚¬ ì œëª©ë“¤ì…ë‹ˆë‹¤. ì´ ê¸°ì‚¬ë“¤ì˜ ê³µí†µ ì£¼ì œë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”.

ê¸°ì‚¬ ì œëª©ë“¤:
{titles_text}

ì‘ë‹µ í˜•ì‹:
1. ì£¼ì œ: (í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½)
2. í‚¤ì›Œë“œ: (3-5ê°œ)
3. ë¶„ì•¼: (ì •ì¹˜/ê²½ì œ/ì‚¬íšŒ/êµ­ì œ/ë¬¸í™” ë“±)
"""
            
            try:
                response = self.openai_client.chat.completions.create(
                    model="gpt-4",
                    messages=[{"role": "user", "content": prompt}],
                    max_tokens=200,
                    temperature=0.3
                )
                
                analysis = response.choices[0].message.content
                cluster_topics[cluster_id] = {
                    'topic_analysis': analysis,
                    'article_count': len(articles),
                    'articles': articles
                }
                
            except Exception as e:
                print(f"âŒ í´ëŸ¬ìŠ¤í„° {cluster_id} ë¶„ì„ ì‹¤íŒ¨: {e}")
                cluster_topics[cluster_id] = {
                    'topic_analysis': "ë¶„ì„ ì‹¤íŒ¨",
                    'article_count': len(articles),
                    'articles': articles
                }
        
        return cluster_topics
    
    def analyze_media_bias(self, cluster_topics):
        """í´ëŸ¬ìŠ¤í„°ë³„ ì–¸ë¡ ì‚¬ í¸í–¥ ë¶„ì„"""
        print(f"\nâš–ï¸ ì–¸ë¡ ì‚¬ë³„ í¸í–¥ ë¶„ì„ ì¤‘...")
        
        bias_analysis = {}
        
        for cluster_id, cluster_data in cluster_topics.items():
            articles = cluster_data['articles']
            
            # ì–¸ë¡ ì‚¬ë³„ ê¸°ì‚¬ ìˆ˜ ì§‘ê³„
            media_count = {}
            for article in articles:
                media_name = article['media_outlets']['name']
                media_bias = article['media_outlets']['bias']
                
                if media_name not in media_count:
                    media_count[media_name] = {
                        'count': 0,
                        'bias': media_bias,
                        'titles': []
                    }
                media_count[media_name]['count'] += 1
                media_count[media_name]['titles'].append(article['title'])
            
            # í¸í–¥ë³„ ì§‘ê³„
            bias_summary = {'left': 0, 'center': 0, 'right': 0}
            for media, data in media_count.items():
                bias_summary[data['bias']] += data['count']
            
            bias_analysis[cluster_id] = {
                'topic': cluster_data['topic_analysis'],
                'total_articles': len(articles),
                'media_breakdown': media_count,
                'bias_summary': bias_summary
            }
        
        return bias_analysis
    
    def generate_report(self, bias_analysis):
        """ë¶„ì„ ê²°ê³¼ ë¦¬í¬íŠ¸ ìƒì„±"""
        print(f"\nğŸ“‹ BlindSpot ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
        
        report = f"""
# BlindSpot ì–¸ë¡  í¸í–¥ ë¶„ì„ ë¦¬í¬íŠ¸
ìƒì„± ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## ğŸ“Š ì „ì²´ ìš”ì•½
- ì´ í´ëŸ¬ìŠ¤í„° ìˆ˜: {len(bias_analysis)}ê°œ
- ë¶„ì„ëœ ê¸°ì‚¬ ìˆ˜: {sum(data['total_articles'] for data in bias_analysis.values())}ê°œ

## ğŸ¯ í´ëŸ¬ìŠ¤í„°ë³„ ìƒì„¸ ë¶„ì„

"""
        
        for cluster_id, data in bias_analysis.items():
            report += f"""
### í´ëŸ¬ìŠ¤í„° {cluster_id}
**ì£¼ì œ ë¶„ì„:**
{data['topic'].replace('1. ', '**ì£¼ì œ:** ').replace('2. ', '**í‚¤ì›Œë“œ:** ').replace('3. ', '**ë¶„ì•¼:** ')}

**ê¸°ì‚¬ ìˆ˜:** {data['total_articles']}ê°œ

**ì–¸ë¡ ì‚¬ë³„ ë¶„í¬:**
"""
            for media, media_data in data['media_breakdown'].items():
                bias_emoji = {'left': 'ğŸ”´', 'center': 'âšª', 'right': 'ğŸ”µ'}[media_data['bias']]
                report += f"- {bias_emoji} {media}: {media_data['count']}ê°œ ({media_data['bias']})\n"
            
            left = data['bias_summary']['left']
            center = data['bias_summary']['center'] 
            right = data['bias_summary']['right']
            total = left + center + right
            
            if total > 0:
                report += f"""
**í¸í–¥ ë¶„ì„:**
- ğŸ”´ ì¢ŒíŒŒ ì„±í–¥: {left}ê°œ ({left/total*100:.1f}%)
- âšª ì¤‘ë¦½ ì„±í–¥: {center}ê°œ ({center/total*100:.1f}%)
- ğŸ”µ ìš°íŒŒ ì„±í–¥: {right}ê°œ ({right/total*100:.1f}%)

**í¸í–¥ íŒì •:** """
                
                if left > right + center:
                    report += "ğŸ”´ ì¢Œí¸í–¥ ìš°ì„¸"
                elif right > left + center:
                    report += "ğŸ”µ ìš°í¸í–¥ ìš°ì„¸"
                elif center > left + right:
                    report += "âšª ì¤‘ë¦½ ìš°ì„¸"
                else:
                    report += "âš–ï¸ ê· í˜•ì  ë³´ë„"
            
            report += "\n---\n"
        
        return report
    
    def run_full_analysis(self, n_clusters=8):
        """ì „ì²´ ë¶„ì„ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        print("ğŸš€ BlindSpot ì–¸ë¡  í¸í–¥ ë¶„ì„ ì‹œì‘!")
        print("=" * 60)
        
        # 1. ê¸°ì‚¬ ë°ì´í„° ë¡œë“œ
        articles = self.load_articles_from_db()
        if not articles:
            print("âŒ ë¶„ì„í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        # 2. í´ëŸ¬ìŠ¤í„°ë§
        result = self.cluster_articles(articles, n_clusters)
        if result is None:
            print("âŒ í´ëŸ¬ìŠ¤í„°ë§ ì‹¤íŒ¨")
            return None
        
        clustered_articles, cluster_centers = result
        
        # 3. í´ëŸ¬ìŠ¤í„° ì£¼ì œ ë¶„ì„
        cluster_topics = self.analyze_cluster_topics(clustered_articles)
        
        # 4. í¸í–¥ ë¶„ì„
        bias_analysis = self.analyze_media_bias(cluster_topics)
        
        # 5. ë¦¬í¬íŠ¸ ìƒì„±
        report = self.generate_report(bias_analysis)
        
        # 6. ê²°ê³¼ ì €ì¥
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        report_filename = f"blindspot_analysis_{timestamp}.md"
        
        with open(report_filename, 'w', encoding='utf-8') as f:
            f.write(report)
        
        print(f"âœ… ë¶„ì„ ì™„ë£Œ! ë¦¬í¬íŠ¸ ì €ì¥: {report_filename}")
        print("\n" + "=" * 60)
        print(report)
        
        return {
            'clustered_articles': clustered_articles,
            'cluster_topics': cluster_topics,
            'bias_analysis': bias_analysis,
            'report': report
        }

# ì‹¤í–‰ìš© í•¨ìˆ˜
def main():
    # OpenAI API í‚¤ ì…ë ¥ ë°›ê¸°
    api_key = input("ğŸ”‘ OpenAI API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”: ").strip()
    
    if not api_key:
        print("âŒ API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        return
    
    # ë¶„ì„ ì‹¤í–‰
    analyzer = BlindSpotAnalyzer(api_key)
    result = analyzer.run_full_analysis(n_clusters=6)  # 6ê°œ í´ëŸ¬ìŠ¤í„°ë¡œ ì‹œì‘
    
    if result:
        print("ğŸ‰ BlindSpot ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")

if __name__ == "__main__":
    main()