from playwright.sync_api import sync_playwright
import time
import sys
import os

# ìƒìœ„ ë””ë ‰í† ë¦¬ì˜ supabase_clientë¥¼ importí•˜ê¸° ìœ„í•´ ê²½ë¡œ ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from supabase_client import save_article_to_db, init_supabase

def crawl_kbs():
    # Supabase ì´ˆê¸°í™”
    supabase = init_supabase()
    print("ğŸ”— Supabase ì—°ê²° ì™„ë£Œ")
    
    categories = [
        {"name": "ì •ì¹˜", "url": "https://news.kbs.co.kr/news/pc/category/category.do?ctcd=0003"},
        {"name": "ê²½ì œ", "url": "https://news.kbs.co.kr/news/pc/category/category.do?ctcd=0004"},
        {"name": "ì‚¬íšŒ", "url": "https://news.kbs.co.kr/news/pc/category/category.do?ctcd=0005"},
    ]
    
    with sync_playwright() as p:
        # ğŸš€ ë¸Œë¼ìš°ì € ìµœì í™” ì˜µì…˜ ì ìš©
        browser = p.chromium.launch(
            headless=True,
            args=[
                '--no-sandbox',
                '--disable-dev-shm-usage', 
                '--disable-images',           # ì´ë¯¸ì§€ ë¡œë”© ì°¨ë‹¨
                '--disable-plugins',
                '--disable-extensions',
                '--no-first-run',
                '--disable-default-apps'
            ]
        )
        page = browser.new_page()

        for category in categories:
            print(f"\n=== KBS {category['name']} ê¸°ì‚¬ í¬ë¡¤ë§ ì‹œì‘ ===")
            
            page.goto(category["url"], wait_until='domcontentloaded')
            time.sleep(3)
            
            # ë” ë§ì€ ê¸°ì‚¬ë¥¼ ë¡œë“œí•˜ê¸° ìœ„í•´ ìŠ¤í¬ë¡¤ ë‹¤ìš´
            for i in range(5):  # 5ë²ˆ ìŠ¤í¬ë¡¤
                page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                time.sleep(1)
                
                # ë”ë³´ê¸° ë²„íŠ¼ì´ ìˆë‹¤ë©´ í´ë¦­
                try:
                    more_button = page.query_selector("button:has-text('ë”ë³´ê¸°'), .more-btn, .btn-more")
                    if more_button and more_button.is_visible():
                        more_button.click()
                        time.sleep(2)
                        print(f"ë”ë³´ê¸° ë²„íŠ¼ í´ë¦­ {i+1}/5")
                except:
                    pass
            
            # KBS ê¸°ì‚¬ ë§í¬ ìˆ˜ì§‘ (ì—¬ëŸ¬ ì…€ë ‰í„° ì‹œë„)
            article_selectors = [
                "a[href*='/news/view/']",  # KBS ê¸°ì‚¬ URL íŒ¨í„´
                "a[href*='ncd=']",         # KBS ë‰´ìŠ¤ ì½”ë“œ íŒ¨í„´  
                ".headline a",
                ".news-item a",
                ".title a",
                "h3 a", 
                "h4 a",
                ".subject a"
            ]
            
            article_urls = []
            visited_urls = set()
            
            for selector in article_selectors:
                links = page.query_selector_all(selector)
                if links:
                    print(f"'{selector}' ì…€ë ‰í„°ë¡œ {len(links)}ê°œ ë§í¬ ë°œê²¬")
                    
                    for link in links:
                        try:
                            href = link.get_attribute("href")
                            if href and ("/news/view/" in href or "ncd=" in href):
                                if not href.startswith("http"):
                                    href = "https://news.kbs.co.kr" + href
                                
                                if href not in visited_urls:
                                    article_urls.append(href)
                                    visited_urls.add(href)
                                    
                                if len(article_urls) >= 50:  # ì—¬ìœ ìˆê²Œ 50ê°œê¹Œì§€ ìˆ˜ì§‘
                                    break
                        except:
                            continue
                    
                    if article_urls:
                        break  # ë§í¬ë¥¼ ì°¾ìœ¼ë©´ ë‹¤ë¥¸ ì…€ë ‰í„°ëŠ” ì‹œë„í•˜ì§€ ì•ŠìŒ
            
            print(f"ìˆ˜ì§‘ëœ ê¸°ì‚¬ URL: {len(article_urls)}ê°œ")
            
            # ìƒìœ„ 30ê°œ ê¸°ì‚¬ë§Œ ì²˜ë¦¬
            target_urls = article_urls[:30]
            articles_collected = 0
            
            for i, article_url in enumerate(target_urls):
                try:
                    print(f"ê¸°ì‚¬ {i+1}/{len(target_urls)} í¬ë¡¤ë§ ì¤‘...")
                    
                    page.goto(article_url, wait_until='domcontentloaded')
                    time.sleep(1)
                    
                    # ì œëª© ì¶”ì¶œ
                    title = page.title()
                    
                    # ë³¸ë¬¸ ì¶”ì¶œ (ì—¬ëŸ¬ ì…€ë ‰í„° ì‹œë„)
                    content_selectors = [
                        ".detail-body",
                        ".article-body", 
                        ".news-content",
                        ".content-area",
                        ".article-text",
                        ".view-cont",
                        ".txt",
                        "#content",
                        ".article_txt"
                    ]
                    
                    content = ""
                    for content_selector in content_selectors:
                        try:
                            content_element = page.query_selector(content_selector)
                            if content_element:
                                content = content_element.inner_text().strip()
                                if len(content) > 100:  # ì¶©ë¶„í•œ ê¸¸ì´ì˜ ë³¸ë¬¸
                                    break
                        except:
                            continue
                    
                    # ìœ íš¨í•œ ê¸°ì‚¬ì¸ì§€ í™•ì¸ í›„ DB ì €ì¥
                    if title and content and len(content) > 100:
                        success = save_article_to_db(
                            supabase=supabase,
                            title=title,
                            content=content,
                            url=article_url,
                            media_outlet="KBSë‰´ìŠ¤",
                            category=category["name"]
                        )
                        
                        if success:
                            articles_collected += 1
                            print(f"âœ… {articles_collected}ë²ˆì§¸ ê¸°ì‚¬ ì €ì¥: {title[:50]}...")
                            
                            if articles_collected >= 30:
                                print(f"âœ… {category['name']} ì¹´í…Œê³ ë¦¬ 30ê°œ ì™„ë£Œ!")
                                break
                    else:
                        print(f"âŒ ë³¸ë¬¸ì´ ë¶€ì¡±í•œ ê¸°ì‚¬ ê±´ë„ˆëœ€")
                        
                except Exception as e:
                    print(f"âŒ ê¸°ì‚¬ ì²˜ë¦¬ ì¤‘ ì—ëŸ¬: {str(e)[:50]}")
                    continue
            
            print(f"âœ… KBS {category['name']} ì™„ë£Œ: {articles_collected}ê°œ ìˆ˜ì§‘")

        browser.close()
        
        print(f"\n=== KBS ì´ 90ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ ===")
        print(" ì •ì¹˜: 30ê°œ")
        print(" ê²½ì œ: 30ê°œ")
        print(" ì‚¬íšŒ: 30ê°œ")

if __name__ == "__main__":
    crawl_kbs()
