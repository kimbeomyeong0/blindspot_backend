from playwright.sync_api import sync_playwright
import time
import sys
import os
from concurrent.futures import ThreadPoolExecutor
import traceback

# ìƒìœ„ ë””ë ‰í† ë¦¬ì˜ supabase ëª¨ë“ˆ import
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from supabase_utils import get_supabase_client, save_article_to_db

def crawl_hani():
    # í•œê²¨ë ˆ ì¹´í…Œê³ ë¦¬ë³„ URL
    categories = [
        {"name": "ì •ì¹˜", "prefix": "https://www.hani.co.kr/arti/politics"},
        {"name": "ê²½ì œ", "prefix": "https://www.hani.co.kr/arti/economy"},
        {"name": "ì‚¬íšŒ", "prefix": "https://www.hani.co.kr/arti/society"},
    ]

    # Supabase í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    supabase = get_supabase_client()
    print("ğŸ”— Supabase ì—°ê²° ì™„ë£Œ")

    with sync_playwright() as p:
        # ë¸Œë¼ìš°ì € ìµœì í™” ì˜µì…˜ ì ìš©
        browser = p.chromium.launch(
            headless=True,
            args=[
                '--no-sandbox',
                '--disable-dev-shm-usage', 
                '--disable-images',
                '--disable-plugins',
                '--disable-extensions',
                '--no-first-run',
                '--disable-default-apps'
            ]
        )
        page = browser.new_page()
        all_articles = []

        # ì¹´í…Œê³ ë¦¬ë³„ ìˆœì°¨ í¬ë¡¤ë§
        for category in categories:
            page = browser.new_page()
            try:
                print(f"=== í•œê²¨ë ˆ {category['name']} ê¸°ì‚¬ í¬ë¡¤ë§ ì‹œì‘ ===")
                articles_collected = 0
                page_num = 1
                visited_urls = set()
                while articles_collected < 30 and page_num <= 5:
                    url = f"{category['prefix']}?page={page_num}"
                    print(f"í˜ì´ì§€ ì´ë™: {url}")
                    try:
                        page.goto(url, wait_until='domcontentloaded')
                        time.sleep(1)
                        links = page.query_selector_all("article a")
                        article_urls = []
                        for link in links:
                            try:
                                href = link.get_attribute("href")
                                if href and href.startswith("/arti/"):
                                    full_url = "https://www.hani.co.kr" + href
                                    if full_url not in visited_urls:
                                        article_urls.append(full_url)
                                        visited_urls.add(full_url)
                            except Exception as e:
                                print(f"URL ì¶”ì¶œ ì—ëŸ¬: {e}")
                                traceback.print_exc()
                                continue
                        print(f"ì´ í˜ì´ì§€ì—ì„œ {len(article_urls)}ê°œ ìƒˆë¡œìš´ ê¸°ì‚¬ URL ë°œê²¬")
                        for article_url in article_urls:
                            if articles_collected >= 30:
                                break
                            try:
                                page.goto(article_url, wait_until='domcontentloaded')
                                time.sleep(1)
                                title = page.title()
                                content = ""
                                try:
                                    content_element = page.query_selector(".article-text")
                                    if content_element:
                                        content = content_element.inner_text()
                                except Exception as e:
                                    print(f"ë³¸ë¬¸ ì¶”ì¶œ ì—ëŸ¬: {e}")
                                    traceback.print_exc()
                                if title and content:
                                    article_data = {
                                        "title": title,
                                        "content": content,
                                        "category": category["name"],
                                        "url": article_url,
                                        "media_outlet": "í•œê²¨ë ˆ",
                                        "bias": "left"
                                    }
                                    if save_article_to_db(supabase, article_data):
                                        all_articles.append(article_data)
                                        articles_collected += 1
                                        print(f"âœ… {articles_collected}ë²ˆì§¸ ê¸°ì‚¬ ì €ì¥: {title[:50]}...")
                                    else:
                                        print(f"âŒ DB ì €ì¥ ì‹¤íŒ¨: {title[:50]}...")
                            except Exception as e:
                                print(f"âŒ ê¸°ì‚¬ ì²˜ë¦¬ ì¤‘ ì—ëŸ¬: {e}")
                                traceback.print_exc()
                                continue
                        page_num += 1
                    except Exception as e:
                        print(f"í˜ì´ì§€ ë¡œë”© ì—ëŸ¬: {e}")
                        traceback.print_exc()
                        break
                print(f"âœ… í•œê²¨ë ˆ {category['name']} ì™„ë£Œ: {articles_collected}ê°œ ìˆ˜ì§‘")
            finally:
                page.close()

        browser.close()
        return all_articles

# í…ŒìŠ¤íŠ¸ìš© ì‹¤í–‰
if __name__ == "__main__":
    articles = crawl_hani()
    print(f"\n=== í•œê²¨ë ˆ ì´ {len(articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ ===")
    
    # ì¹´í…Œê³ ë¦¬ë³„ ê°œìˆ˜ í™•ì¸
    categories_count = {}
    for article in articles:
        cat = article['category']
        categories_count[cat] = categories_count.get(cat, 0) + 1
    
    for cat, count in categories_count.items():
        print(f" {cat}: {count}ê°œ")