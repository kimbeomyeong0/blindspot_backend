from playwright.sync_api import sync_playwright
import time
import sys
import os
from concurrent.futures import ThreadPoolExecutor
import traceback

# ìƒìœ„ ë””ë ‰í† ë¦¬ì˜ supabase ëª¨ë“ˆ import
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from db import save_article_to_db, init_supabase

def crawl_chosun():
    # Supabase ì´ˆê¸°í™”
    supabase = init_supabase()
    print("ğŸ”— Supabase ì—°ê²° ì™„ë£Œ")
    
    # ì¡°ì„ ì¼ë³´ ì¹´í…Œê³ ë¦¬ë³„ URL
    categories = [
        {"name": "ì •ì¹˜", "url": "https://www.chosun.com/politics/"},
        {"name": "ì‚¬íšŒ", "url": "https://www.chosun.com/national/"},
        {"name": "ê²½ì œ", "url": "https://www.chosun.com/economy/"},
    ]
    
    with sync_playwright() as p:
        browser = p.chromium.launch(
            headless=True,
            args=[
                '--no-sandbox',
                '--disable-dev-shm-usage', 
                '--disable-images',           # ì´ë¯¸ì§€ ë¡œë”© ì°¨ë‹¨
                '--disable-javascript',       # JS ë¹„í™œì„±í™” (í•„ìš”ì‹œ)
                '--disable-plugins',
                '--disable-extensions',
                '--no-first-run',
                '--disable-default-apps'
            ]
        )
        page = browser.new_page()

        # ì¹´í…Œê³ ë¦¬ë³„ ìˆœì°¨ í¬ë¡¤ë§
        for category in categories:
            page = browser.new_page()
            try:
                print(f"\n=== ì¡°ì„ ì¼ë³´ {category['name']} ê¸°ì‚¬ í¬ë¡¤ë§ ì‹œì‘ ===")
                page.goto(category['url'])
                time.sleep(1)
                click_count = 0
                target_articles = 30
                while click_count < 15:
                    try:
                        articles = page.query_selector_all("a[href*='/politics/'], a[href*='/national/'], a[href*='/economy/']")
                        print(f"í˜„ì¬ ë¡œë“œëœ ê¸°ì‚¬ ê°œìˆ˜: {len(articles)}")
                        if len(articles) >= target_articles:
                            print(f"{target_articles}ê°œ ì´ìƒ ê¸°ì‚¬ ë¡œë“œ ì™„ë£Œ!")
                            break
                        more_button_selectors = [
                            ".more-news-btn",
                            ".btn-more",
                            ".load-more",
                            ".more-btn",
                            "button:has-text('ë”ë³´ê¸°')",
                            "a:has-text('ë”ë³´ê¸°')",
                            ".story-card-more",
                            ".list-more"
                        ]
                        clicked = False
                        for selector in more_button_selectors:
                            try:
                                more_button = page.query_selector(selector)
                                if more_button and more_button.is_visible():
                                    print(f"ë”ë³´ê¸° ë²„íŠ¼ í´ë¦­ {click_count + 1}ë²ˆì§¸... (ì…€ë ‰í„°: {selector})")
                                    more_button.click()
                                    time.sleep(1)
                                    click_count += 1
                                    clicked = True
                                    break
                            except Exception as e:
                                print(f"ë”ë³´ê¸° í´ë¦­ ì—ëŸ¬: {e}")
                                traceback.print_exc()
                                continue
                        if not clicked:
                            print("ë”ë³´ê¸° ë²„íŠ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ. ìŠ¤í¬ë¡¤ ì‹œë„...")
                            page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                            time.sleep(0.5)
                            click_count += 1
                    except Exception as e:
                        print(f"ë”ë³´ê¸° í´ë¦­ ì¤‘ ì—ëŸ¬: {e}")
                        traceback.print_exc()
                        break
                article_selectors = [
                    f"a[href*='/{category['url'].split('/')[-2]}/']",
                    ".story-card a",
                    ".headline a",
                    ".news-item a",
                    ".article-link",
                    "h3 a",
                    "h4 a"
                ]
                article_urls = []
                visited_urls = set()
                for selector in article_selectors:
                    try:
                        links = page.query_selector_all(selector)
                        if links:
                            print(f"'{selector}' ì…€ë ‰í„°ë¡œ {len(links)}ê°œ ë§í¬ ë°œê²¬")
                            for link in links:
                                try:
                                    href = link.get_attribute("href")
                                    if href:
                                        if href.startswith("/"):
                                            full_url = "https://www.chosun.com" + href
                                        elif href.startswith("https://www.chosun.com"):
                                            full_url = href
                                        else:
                                            continue
                                        category_path = category['url'].split('/')[-2]
                                        if category_path in full_url and full_url not in visited_urls:
                                            article_urls.append(full_url)
                                            visited_urls.add(full_url)
                                            if len(article_urls) >= 40:
                                                break
                                except Exception as e:
                                    print(f"URL ì¶”ì¶œ ì—ëŸ¬: {e}")
                                    traceback.print_exc()
                                    continue
                            if article_urls:
                                break
                    except Exception as e:
                        print(f"ì…€ë ‰í„° '{selector}' ì²˜ë¦¬ ì¤‘ ì—ëŸ¬: {e}")
                        traceback.print_exc()
                        continue
                print(f"ìˆ˜ì§‘ëœ ê¸°ì‚¬ URL: {len(article_urls)}ê°œ")
                target_urls = article_urls[:30]
                article_count = 0
                for i, article_url in enumerate(target_urls):
                    try:
                        print(f"ê¸°ì‚¬ {i+1}/{len(target_urls)} í¬ë¡¤ë§ ì¤‘...")
                        page.goto(article_url)
                        time.sleep(0.5)
                        title = page.title() or "ì œëª© ì—†ìŒ"
                        content_selectors = [
                            ".article-body",
                            ".news-article-body",
                            ".story-content",
                            "#article-body",
                            ".article-content",
                            ".story-body",
                            ".entry-content"
                        ]
                        content = ""
                        for content_selector in content_selectors:
                            try:
                                content_element = page.query_selector(content_selector)
                                if content_element:
                                    content = content_element.inner_text().strip()
                                    if len(content) > 100:
                                        break
                            except Exception as e:
                                print(f"ë³¸ë¬¸ ì¶”ì¶œ ì—ëŸ¬: {e}")
                                traceback.print_exc()
                                continue
                        if not content:
                            content = "ë³¸ë¬¸ì„ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                        success = save_article_to_db(
                            supabase=supabase,
                            title=title,
                            content=content,
                            url=article_url,
                            media_outlet="ì¡°ì„ ì¼ë³´",
                            category=category['name']
                        )
                        if success:
                            article_count += 1
                            print(f"âœ… {article_count}ë²ˆì§¸ ê¸°ì‚¬ ì €ì¥: {title[:50]}...")
                            if article_count >= 30:
                                break
                    except Exception as e:
                        print(f"âŒ ê¸°ì‚¬ í¬ë¡¤ë§ ì‹¤íŒ¨ {article_url}: {e}")
                        traceback.print_exc()
                        continue
                print(f"âœ… ì¡°ì„ ì¼ë³´ {category['name']} ì™„ë£Œ: {article_count}ê°œ ìˆ˜ì§‘")
            finally:
                page.close()

        browser.close()
        
        print(f"\n=== ì¡°ì„ ì¼ë³´ ì´ 90ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ ===")
        print(" ì •ì¹˜: 30ê°œ")
        print(" ì‚¬íšŒ: 30ê°œ")
        print(" ê²½ì œ: 30ê°œ")

if __name__ == "__main__":
    crawl_chosun()
