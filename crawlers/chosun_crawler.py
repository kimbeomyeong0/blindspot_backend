from playwright.sync_api import sync_playwright
import time
import sys
import os

# ìƒìœ„ ë””ë ‰í† ë¦¬ì˜ supabase_clientë¥¼ importí•˜ê¸° ìœ„í•´ ê²½ë¡œ ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from supabase_client import save_article_to_db, init_supabase

def crawl_chosun():
    # Supabase ì´ˆê¸°í™”
    supabase = init_supabase()
    print("ğŸ”— Supabase ì—°ê²° ì™„ë£Œ")
    
    # ì¡°ì„ ì¼ë³´ ì¹´í…Œê³ ë¦¬ë³„ URL
    categories = [
        {"name": "ì •ì¹˜", "url": "https://www.chosun.com/politics/"},
        {"name": "ì‚¬íšŒ", "url": "https://www.chosun.com/national/"},
        {"name": "ê²½ì œ", "url": "https://www.chosun.com/economy/"},
    ]
    
    with sync_playwright() as p:
        browser = p.chromium.launch(
            headless=True,
            args=[
                '--no-sandbox',
                '--disable-dev-shm-usage', 
                '--disable-images',           # ì´ë¯¸ì§€ ë¡œë”© ì°¨ë‹¨
                '--disable-javascript',       # JS ë¹„í™œì„±í™” (í•„ìš”ì‹œ)
                '--disable-plugins',
                '--disable-extensions',
                '--no-first-run',
                '--disable-default-apps'
            ]
        )
        page = browser.new_page()

        for category in categories:
            print(f"\n=== ì¡°ì„ ì¼ë³´ {category['name']} ê¸°ì‚¬ í¬ë¡¤ë§ ì‹œì‘ ===")
            page.goto(category['url'])
            time.sleep(3)  # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
            
            # ë”ë³´ê¸° ë²„íŠ¼ì„ ì—¬ëŸ¬ ë²ˆ í´ë¦­í•´ì„œ 30ê°œ ì´ìƒ ê¸°ì‚¬ ë¡œë“œ
            click_count = 0
            target_articles = 30
            
            while click_count < 15:  # ìµœëŒ€ 15ë²ˆ ë”ë³´ê¸° í´ë¦­
                try:
                    # í˜„ì¬ ê¸°ì‚¬ ê°œìˆ˜ í™•ì¸ (ì¡°ì„ ì¼ë³´ ê¸°ì‚¬ ë§í¬ íŒ¨í„´)
                    articles = page.query_selector_all("a[href*='/politics/'], a[href*='/national/'], a[href*='/economy/']")
                    print(f"í˜„ì¬ ë¡œë“œëœ ê¸°ì‚¬ ê°œìˆ˜: {len(articles)}")
                    
                    if len(articles) >= target_articles:
                        print(f"{target_articles}ê°œ ì´ìƒ ê¸°ì‚¬ ë¡œë“œ ì™„ë£Œ!")
                        break
                    
                    # ë”ë³´ê¸° ë²„íŠ¼ ì°¾ê¸° ë° í´ë¦­ (ì¡°ì„ ì¼ë³´ ë”ë³´ê¸° ë²„íŠ¼ ì…€ë ‰í„°)
                    more_button_selectors = [
                        ".more-news-btn",
                        ".btn-more", 
                        ".load-more",
                        ".more-btn",
                        "button:has-text('ë”ë³´ê¸°')",
                        "a:has-text('ë”ë³´ê¸°')",
                        ".story-card-more",
                        ".list-more"
                    ]
                    
                    clicked = False
                    for selector in more_button_selectors:
                        try:
                            more_button = page.query_selector(selector)
                            if more_button and more_button.is_visible():
                                print(f"ë”ë³´ê¸° ë²„íŠ¼ í´ë¦­ {click_count + 1}ë²ˆì§¸... (ì…€ë ‰í„°: {selector})")
                                more_button.click()
                                time.sleep(3)  # ë¡œë”© ëŒ€ê¸°
                                click_count += 1
                                clicked = True
                                break
                        except Exception as e:
                            continue
                    
                    if not clicked:
                        print("ë”ë³´ê¸° ë²„íŠ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ. ìŠ¤í¬ë¡¤ ì‹œë„...")
                        # ìŠ¤í¬ë¡¤ë¡œ ì¶”ê°€ ì½˜í…ì¸  ë¡œë“œ ì‹œë„
                        page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                        time.sleep(2)
                        click_count += 1
                        
                except Exception as e:
                    print(f"ë”ë³´ê¸° í´ë¦­ ì¤‘ ì—ëŸ¬: {e}")
                    break
            
            # ê¸°ì‚¬ ë§í¬ ìˆ˜ì§‘ (ì¡°ì„ ì¼ë³´ URL íŒ¨í„´)
            article_selectors = [
                f"a[href*='/{category['url'].split('/')[-2]}/']",  # í•´ë‹¹ ì¹´í…Œê³ ë¦¬ URL íŒ¨í„´
                ".story-card a",
                ".headline a", 
                ".news-item a",
                ".article-link",
                "h3 a",
                "h4 a"
            ]
            
            article_urls = []
            visited_urls = set()
            
            for selector in article_selectors:
                try:
                    links = page.query_selector_all(selector)
                    if links:
                        print(f"'{selector}' ì…€ë ‰í„°ë¡œ {len(links)}ê°œ ë§í¬ ë°œê²¬")
                        
                        for link in links:
                            try:
                                href = link.get_attribute("href")
                                if href:
                                    # ìƒëŒ€ URLì„ ì ˆëŒ€ URLë¡œ ë³€í™˜
                                    if href.startswith("/"):
                                        full_url = "https://www.chosun.com" + href
                                    elif href.startswith("https://www.chosun.com"):
                                        full_url = href
                                    else:
                                        continue
                                    
                                    # í•´ë‹¹ ì¹´í…Œê³ ë¦¬ ê¸°ì‚¬ì¸ì§€ í™•ì¸
                                    category_path = category['url'].split('/')[-2]  # politics, national, economy
                                    if category_path in full_url and full_url not in visited_urls:
                                        article_urls.append(full_url)
                                        visited_urls.add(full_url)
                                        
                                        if len(article_urls) >= 40:  # ì—¬ìœ ìˆê²Œ 40ê°œê¹Œì§€
                                            break
                            except:
                                continue
                        
                        if article_urls:
                            break  # ë§í¬ë¥¼ ì°¾ìœ¼ë©´ ë‹¤ë¥¸ ì…€ë ‰í„°ëŠ” ì‹œë„í•˜ì§€ ì•ŠìŒ
                except Exception as e:
                    print(f"ì…€ë ‰í„° '{selector}' ì²˜ë¦¬ ì¤‘ ì—ëŸ¬: {e}")
                    continue
            
            print(f"ìˆ˜ì§‘ëœ ê¸°ì‚¬ URL: {len(article_urls)}ê°œ")
            
            # ìƒìœ„ 30ê°œ ê¸°ì‚¬ë§Œ ì²˜ë¦¬
            target_urls = article_urls[:30]
            article_count = 0
            
            # ê° ê¸°ì‚¬ í˜ì´ì§€ì—ì„œ ë³¸ë¬¸ ì¶”ì¶œ
            for i, article_url in enumerate(target_urls):
                try:
                    print(f"ê¸°ì‚¬ {i+1}/{len(target_urls)} í¬ë¡¤ë§ ì¤‘...")
                    page.goto(article_url)
                    time.sleep(2)
                    
                    # ì¡°ì„ ì¼ë³´ ê¸°ì‚¬ ì œëª©ê³¼ ë³¸ë¬¸ ì…€ë ‰í„°
                    title = page.title() or "ì œëª© ì—†ìŒ"
                    
                    # ë³¸ë¬¸ ì¶”ì¶œ (ì¡°ì„ ì¼ë³´ ë³¸ë¬¸ ì…€ë ‰í„°)
                    content_selectors = [
                        ".article-body",
                        ".news-article-body", 
                        ".story-content",
                        "#article-body",
                        ".article-content",
                        ".story-body",
                        ".entry-content"
                    ]
                    
                    content = ""
                    for content_selector in content_selectors:
                        try:
                            content_element = page.query_selector(content_selector)
                            if content_element:
                                content = content_element.inner_text().strip()
                                if len(content) > 100:  # ì¶©ë¶„í•œ ê¸¸ì´ì˜ ë³¸ë¬¸
                                    break
                        except:
                            continue
                    
                    if not content:
                        content = "ë³¸ë¬¸ì„ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                    
                    # DBì— ì €ì¥
                    success = save_article_to_db(
                        supabase=supabase,
                        title=title,
                        content=content,
                        url=article_url,
                        media_outlet="ì¡°ì„ ì¼ë³´",
                        category=category['name']
                    )
                    
                    if success:
                        article_count += 1
                        print(f"âœ… {article_count}ë²ˆì§¸ ê¸°ì‚¬ ì €ì¥: {title[:50]}...")
                        
                        # 30ê°œ ìˆ˜ì§‘í•˜ë©´ ì¢…ë£Œ
                        if article_count >= 30:
                            break
                    
                except Exception as e:
                    print(f"âŒ ê¸°ì‚¬ í¬ë¡¤ë§ ì‹¤íŒ¨ {article_url}: {e}")
                    continue
            
            print(f"âœ… ì¡°ì„ ì¼ë³´ {category['name']} ì™„ë£Œ: {article_count}ê°œ ìˆ˜ì§‘")
        
        browser.close()
        
        print(f"\n=== ì¡°ì„ ì¼ë³´ ì´ 90ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ ===")
        print(" ì •ì¹˜: 30ê°œ")
        print(" ì‚¬íšŒ: 30ê°œ")
        print(" ê²½ì œ: 30ê°œ")

if __name__ == "__main__":
    crawl_chosun()
