from playwright.sync_api import sync_playwright
import time
import sys
import os

# ìƒìœ„ ë””ë ‰í† ë¦¬ì˜ supabase_client ëª¨ë“ˆ import
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from supabase_client import get_supabase_client, save_article_to_db

def crawl_hani():
    # í•œê²¨ë ˆ ì¹´í…Œê³ ë¦¬ë³„ URL
    categories = [
        {"name": "ì •ì¹˜", "prefix": "https://www.hani.co.kr/arti/politics"},
        {"name": "ê²½ì œ", "prefix": "https://www.hani.co.kr/arti/economy"},
        {"name": "ì‚¬íšŒ", "prefix": "https://www.hani.co.kr/arti/society"},
    ]

    # Supabase í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    supabase = get_supabase_client()
    print("ğŸ”— Supabase ì—°ê²° ì™„ë£Œ")

    with sync_playwright() as p:
        # ë¸Œë¼ìš°ì € ìµœì í™” ì˜µì…˜ ì ìš©
        browser = p.chromium.launch(
            headless=True,
            args=[
                '--no-sandbox',
                '--disable-dev-shm-usage', 
                '--disable-images',
                '--disable-plugins',
                '--disable-extensions',
                '--no-first-run',
                '--disable-default-apps'
            ]
        )
        page = browser.new_page()
        all_articles = []

        for category in categories:
            print(f"=== í•œê²¨ë ˆ {category['name']} ê¸°ì‚¬ í¬ë¡¤ë§ ì‹œì‘ ===")
            articles_collected = 0
            page_num = 1
            visited_urls = set()
            
            while articles_collected < 30 and page_num <= 5:  # ìµœëŒ€ 5í˜ì´ì§€
                url = f"{category['prefix']}?page={page_num}"
                print(f"í˜ì´ì§€ ì´ë™: {url}")
                
                try:
                    page.goto(url, wait_until='domcontentloaded')
                    time.sleep(2)
                    
                    # í˜„ì¬ í˜ì´ì§€ì—ì„œ ëª¨ë“  ê¸°ì‚¬ URLì„ ë¨¼ì € ì¶”ì¶œ
                    links = page.query_selector_all("article a")
                    article_urls = []
                    
                    for link in links:
                        try:
                            href = link.get_attribute("href")
                            if href and href.startswith("/arti/"):
                                full_url = "https://www.hani.co.kr" + href
                                if full_url not in visited_urls:
                                    article_urls.append(full_url)
                                    visited_urls.add(full_url)
                        except:
                            continue
                    
                    print(f"ì´ í˜ì´ì§€ì—ì„œ {len(article_urls)}ê°œ ìƒˆë¡œìš´ ê¸°ì‚¬ URL ë°œê²¬")
                    
                    # ì¶”ì¶œëœ URLë“¤ë¡œ ê°œë³„ ê¸°ì‚¬ ìˆ˜ì§‘
                    for article_url in article_urls:
                        if articles_collected >= 30:
                            break
                            
                        try:
                            page.goto(article_url, wait_until='domcontentloaded')
                            time.sleep(1)
                            
                            title = page.title()
                            content = ""
                            
                            try:
                                content_element = page.query_selector(".article-text")
                                if content_element:
                                    content = content_element.inner_text()
                            except:
                                pass
                            
                            if title and content:
                                article_data = {
                                    "title": title,
                                    "content": content,
                                    "category": category["name"],
                                    "url": article_url,
                                    "media_outlet": "í•œê²¨ë ˆ",
                                    "bias": "left"
                                }
                                
                                # ğŸ”¥ Supabaseì— ì €ì¥
                                if save_article_to_db(supabase, article_data):
                                    all_articles.append(article_data)
                                    articles_collected += 1
                                    print(f"âœ… {articles_collected}ë²ˆì§¸ ê¸°ì‚¬ ì €ì¥: {title[:50]}...")
                                else:
                                    print(f"âŒ DB ì €ì¥ ì‹¤íŒ¨: {title[:50]}...")
                            
                        except Exception as e:
                            print(f"âŒ ê¸°ì‚¬ ì²˜ë¦¬ ì¤‘ ì—ëŸ¬ (ë¬´ì‹œ): {str(e)[:50]}...")
                            continue
                    
                    page_num += 1
                    
                except Exception as e:
                    print(f"í˜ì´ì§€ ë¡œë”© ì—ëŸ¬: {e}")
                    break
            
            print(f"âœ… í•œê²¨ë ˆ {category['name']} ì™„ë£Œ: {articles_collected}ê°œ ìˆ˜ì§‘")

        browser.close()
        return all_articles

# í…ŒìŠ¤íŠ¸ìš© ì‹¤í–‰
if __name__ == "__main__":
    articles = crawl_hani()
    print(f"\n=== í•œê²¨ë ˆ ì´ {len(articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ ===")
    
    # ì¹´í…Œê³ ë¦¬ë³„ ê°œìˆ˜ í™•ì¸
    categories_count = {}
    for article in articles:
        cat = article['category']
        categories_count[cat] = categories_count.get(cat, 0) + 1
    
    for cat, count in categories_count.items():
        print(f" {cat}: {count}ê°œ")